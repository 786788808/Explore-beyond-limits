### 目录：
- 1. 项目背景及目的
- 2. 数据探索(EDA)
- 3. 数据清洗
- 4. 建模与模型评价分析
- 5. 总结

### 1. 项目背景及目的
#### 1.1 背景
在kaggle上看到一个关于预测流失用户的项目：  
某银行信用卡业务的客户流失问题日益显著，其银行经理为此很困扰。他们希望有人能帮助预测哪部分客户即将流失，从而能提前采取有效措施挽回这部分客户，让客户留存下来。  
现有该业务的一个数据集，包含 10,000 多名客户信息，字段如下：年龄、薪水、婚姻状况、信用卡额度、信用卡类型等等，共计 18 个字段。  
数据集里只有 16.7% 的客户是流失客户，存在类别不均衡问题。  
>  
[数据获取地址](https://www.kaggle.com/sakshigoyal7/credit-card-customers/notebooks?datasetId=982921&sortBy=voteCount)  
>
#### 1.2 目的
- 建模预测即将流失用户  
- 找出关键影响流失的因素，为银行经理提供相关建议  
>
### 2. 数据探索(EDA)
分类别型和数值型特征两大块做初步的了解,先导入相关包与读入数据：
```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Author  : Hush
# @Software: PyCharm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = 'SimHei'
plt.rcParams['axes.unicode_minus'] = False
import seaborn as sns  
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  # 标准化
from imblearn.over_sampling import SMOTE  # 样本不均衡，用上采样
from collections import Counter
from sklearn.ensemble import RandomForestClassifier  # 选特征时用
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, f1_score, accuracy_score

bank_churners_data = pd.read_csv(r'C:\Users\Administrator\Desktop\credit card customers\BankChurners.csv')
pd.set_option('display.max_columns', None)  # 显示所有列
pd.set_option('display.max_rows', None)  # 显示所有行
print(bank_churners_data.sample(5))
print(bank_churners_data.info())
print(bank_churners_data.describe(include='O'))  # 查看离散型分布情况
```

可以看到，数据集里一共有 10,127 行数据，23 个特征，缺失值以'unknown'字符串存在于数据集，数据类型有数值整型、数值浮点型、字符串。最后两列的数据、客户账号对数据分析没有实际作用，需要删除。
```
bank_churners_data.drop(columns=['CLIENTNUM',
                                 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',
                                 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],
                        inplace=True)
# print(bank_churners_data.info())
print('——————客户流失情况：——————')
print(bank_churners_data['Attrition_Flag'].value_counts())
churn = bank_churners_data['Attrition_Flag'].value_counts()
plt.pie(churn.values, labels=churn.index, autopct='%1.2f%%')
plt.title('客户流失与留存情况')
plt.show()
```
![](https://ae04.alicdn.com/kf/Ud513178d9a7b428cac4484e16778dd6a7.jpg)   
流失客户占比不到16.1%，虽然类别失衡还没达到10倍数，但是后面还是要通过 Smote 来改善类别不平衡问题。  

下面了解一下各特征对客户流失问题的影响：  
#### 2.1 类别型特征
##### (2.1.1) 性别对客户流失的影响
```
print('客户性别分布情况：\n', bank_churners_data['Gender'].value_counts(ascending=True))
gen = bank_churners_data['Gender'].value_counts(ascending=True)
plt.figure(figsize=(10, 4))
plt.subplot(121)
plt.pie(gen.values, labels=gen.index, autopct='%1.2f%%')
plt.title('性别占比', fontsize=10)
plt.subplot(122)
sns.countplot(x='Attrition_Flag', data=bank_churners_data, hue='Gender', palette='Set3')
plt.xlabel('客户类型')
plt.ylabel('人数')
plt.title('各类别客户性别人数', fontsize=10)

plt.suptitle('性别对客户流失的影响')   # 添加一个总标题，前面的是小标题
plt.show()
```
![](https://ae04.alicdn.com/kf/U907864dd56174f34b37d67904fb4a316A.jpg)    
从图看出，女性客户占比较大，占总客户数的53%，比男性多5个百分点。  
在流失或留存客户群体中，女性客户数均比男性客户稍微多一点。  
单看此因素，性别因素并没有很明显地影响客户的流失。   


##### (2.1.2) 教育水平对客户流失的影响
横坐标从多到少降序排位，给后面画图提供排序依据  
```
plt.figure(figsize=(10, 5))
plt.subplot(121)
edu = bank_churners_data['Education_Level'].value_counts(ascending=False)
edu_percent = bank_churners_data['Education_Level'].value_counts(normalize=True, ascending=False)
print(edu)
print(edu_percent)
sns.countplot(x='Education_Level', data=bank_churners_data, order=edu.index, palette='Set3')
# plt.text(x=[1,2,3,4,5,6,7] , y=[1,2,3,4,5,6,7], s=edu_percent.values)  # 暂时不会加数据标签
plt.xlabel('教育水平')
plt.ylabel('人数')
plt.title('教育水平分布', fontsize=10)
plt.xticks(rotation=-25)

plt.subplot(122)
sns.countplot(x='Education_Level', data=bank_churners_data, hue='Attrition_Flag', order=edu.index, palette='Set3')
plt.xlabel('教育水平')
plt.ylabel('人数')
plt.title('各类别客户教育水平', fontsize=10)
plt.xticks(rotation=-25)

plt.suptitle('教育水平对客户流失的影响')
plt.show()
```
![](https://ae03.alicdn.com/kf/U1bec1bc481344daf8e09543e9fd16196w.jpg)    
Graduate 和 High school 这两部分占到总体的 50%，有 15% 学历未知，15% 没有接受过教育。其中，至少保证有 70% 的人都接受过教育。   
单看此因素，教育水平没有很明显地影响客户的流失。     


##### (2.1.3) 婚姻状况对客户流失的影响
```
plt.figure(figsize=(10, 5))
plt.subplot(121)
edu = bank_churners_data['Marital_Status'].value_counts(ascending=False)
edu_percent = bank_churners_data['Marital_Status'].value_counts(normalize=True, ascending=False)
print(edu)
print(edu_percent)
sns.countplot(x='Marital_Status', data=bank_churners_data, order=edu.index, palette='Set3')
# plt.text(x=[1,2,3,4,5,6,7] , y=[1,2,3,4,5,6,7], s=edu_percent.values)  # 暂时不会加数据标签
plt.xlabel('婚姻状况')
plt.ylabel('人数')
plt.title('婚姻状况分布', fontsize=10)
# plt.xticks(rotation=-25)

plt.subplot(122)
sns.countplot(x='Marital_Status', data=bank_churners_data, hue='Attrition_Flag', order=edu.index, palette='Set3')
plt.xlabel('婚姻状况')
plt.ylabel('人数')
plt.title('各类别客户婚姻状况', fontsize=10)
# plt.xticks(rotation=-25)

plt.suptitle('婚姻状况对客户流失的影响')
plt.show()
```
![](https://ae03.alicdn.com/kf/U52cbf29ff69f49288dda57df09256c066.jpg)     
已经结婚的客户占 46%,接近一半，单身客户占 39%,有 7% 未知,其余 7% 是已经离婚的。  
单看此因素，婚姻状况没有很明显地影响客户的流失。     


##### (2.1.4) 收入水平对客户流失的影响
```
plt.figure(figsize=(10, 5))
plt.subplot(121)
edu = bank_churners_data['Income_Category'].value_counts(ascending=False)
edu_percent = bank_churners_data['Income_Category'].value_counts(normalize=True, ascending=False)
print(edu)
print(edu_percent)
sns.countplot(x='Income_Category', data=bank_churners_data, order=edu.index, palette='Set3')
# plt.text(x=[1,2,3,4,5,6,7] , y=[1,2,3,4,5,6,7], s=edu_percent.values)  # 暂时不会加数据标签
plt.xlabel('收入水平')
plt.ylabel('人数')
plt.title('收入水平情况', fontsize=10)
plt.xticks(rotation=-25)

plt.subplot(122)
sns.countplot(x='Income_Category', data=bank_churners_data, hue='Attrition_Flag', order=edu.index, palette='Set3')
plt.xlabel('收入水平')
plt.ylabel('人数')
plt.title('各类别客户收入水平情况', fontsize=10)
plt.xticks(rotation=-25)

plt.suptitle('收入水平对客户流失的影响')
plt.show()
```
![](https://ae04.alicdn.com/kf/U5867f8e091a94bb4af1eddea860de2715.jpg)    
可以看到，收入少于 40k 的客户占 35%，40-60K 的占 18%，80-120K 占 15%，60-80K 的占 14%，120K+ 的占 7%，不知道收入水平的占 11%。   
单看此因素，收入水平没有很明显地影响客户的流失。      


##### (2.1.5) 信用卡级别的对客户流失的影响
```
plt.figure(figsize=(10, 5))
plt.subplot(121)
edu = bank_churners_data['Card_Category'].value_counts(ascending=False)
edu_percent = bank_churners_data['Card_Category'].value_counts(normalize=True, ascending=False)
print(edu)
print(edu_percent)
sns.countplot(x='Card_Category', data=bank_churners_data, order=edu.index, palette='Set3')
# plt.text(x=[1,2,3,4,5,6,7] , y=[1,2,3,4,5,6,7], s=edu_percent.values)  # 暂时不会加数据标签
plt.xlabel('信用卡类型')
plt.ylabel('人数')
plt.title('信用卡类型', fontsize=10)
plt.xticks(rotation=-25)

plt.subplot(122)
sns.countplot(x='Card_Category', data=bank_churners_data, hue='Attrition_Flag', order=edu.index, palette='Set3')
plt.xlabel('信用卡类型')
plt.ylabel('人数')
plt.title('各类别客户信用卡类型', fontsize=10)
plt.xticks(rotation=-25)

plt.suptitle('信用卡类型对客户流失的影响')
plt.show()
```
![](https://ae04.alicdn.com/kf/U30982a9b299049ce9e326fa59c5859770.jpg)   
有 93% 的客户持有的都是蓝卡，占比较大，银卡、金卡、铂金卡合计仅占 7%。   
单看此因素，信用卡类型没有很明显地影响客户的流失。        


#### 2.2 数值型特征
- Customer_Age:客户年龄 the age of Customer
- Dependent_count:相关联的账号数量 Number of dependents
- Months_on_book: Period of relationship with bank
- Total_Relationship_Count: Total no. of products held by the customer
- Months_Inactive_12_mon: No. of months inactive in the last 12 months
- Contacts_Count_12_mon: No. of Contacts in the last 12 months
- Credit_Limit: Credit Limit on the Credit Card
- Total_Revolving_Bal: Total Revolving Balance on the Credit Card
- Avg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months)
- Total_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1)
- Total_Trans_Amt: Total Transaction Amount (Last 12 months)
- Total_Trans_Ct: Total Transaction Count (Last 12 months)
- Total_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1)
- Avg_Utilization_Ratio: Average Card Utilization Ratio
>
##### (2.2.1) 年龄对客户流失的影响
```
print('客户年龄情况：', bank_churners_data['Customer_Age'].describe())
plt.figure(figsize=(12, 4))
plt.subplot(131)
sns.distplot(bank_churners_data['Customer_Age'],
             kde_kws={'label': 'KDE 核密度曲线', 'linestyle': '--'})  # 设置核密度曲线
plt.xlabel('年龄')
plt.title('年龄分布', fontsize=10)

plt.subplot(132)
sns.boxplot(x='Attrition_Flag', y='Customer_Age', data=bank_churners_data,
            width=0.7, palette='Set3')
plt.xlabel('客户类型')
plt.ylabel('年龄')
plt.title('不同类别客户年龄分布(1)', fontsize=10)

plt.subplot(133)
sns.violinplot(x='Attrition_Flag', y='Customer_Age', data=bank_churners_data, palette='Set3')
plt.xlabel('客户类型')
plt.ylabel('年龄')
plt.title('不同类别客户年龄分布(2)', fontsize=10)

plt.suptitle('年龄对客户流失的影响')
plt.show()
```
![](https://ae02.alicdn.com/kf/Ub4b82480702e4604843657fb7a7490fca.jpg)  
看到，客户的年龄接近正态分布,最小年龄为 26，最大年龄为 73，平均数和中位数都是 46，41-52 岁的客户占到总人数的 50%；    
从箱线图和小提琴图可以看到，流失客户的年龄中位数会比留存客户的大，但总的来看，年龄因素没有很明显地影响客户的流失。     


##### (2.2.2) 相关联账户数量对客户流失的影响
```
print('相关联账户数量：', bank_churners_data['Dependent_count'].describe())
plt.figure(figsize=(12, 4))
plt.subplot(131)
sns.distplot(bank_churners_data['Dependent_count'], kde=False)
plt.xlabel('相关联账户数')
plt.ylabel('人数')
plt.title('相关联账户数量分布', fontsize=10)

plt.subplot(132)
sns.boxplot(x='Attrition_Flag', y='Dependent_count', data=bank_churners_data,
            width=0.7, palette='Set3')
plt.xlabel('客户类型')
plt.ylabel('相关联账户数')
plt.title('相关联账户数分布(1)', fontsize=10)

plt.subplot(133)
sns.violinplot(x='Attrition_Flag', y='Dependent_count', data=bank_churners_data, palette='Set3')
plt.xlabel('客户类型')
plt.ylabel('相关联账户数')
plt.title('相关联账户数分布(2)', fontsize=10)

plt.suptitle('相关联账户数对客户流失的影响')
plt.show()
```
![](https://sc01.alicdn.com/kf/U5d60a3a1742c4a93998400f26cff12934.jpg)     
每人平均拥有 2-3 个相关联账户，中位数是 2。有 50% 的客户拥有相关联账户数在 1-3 个。   
分类别来看，流失用户有 50% 集中于 2-3 个相关联的账户(箱线图箱体比较扁)，存在个别异常值。流失用户有 50% 客户集中于 1-3 个相关账户。   
总的来看，相关联账户数没有很明显地影响客户的流失。   


##### (2.2.3) 其余数值型特征对客户流失的影响
看看其余12个数值型特征:
```
other_numerical_fea = ['Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon',
                       'Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1',
                       'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']
# print(len(other_numerical_fea))

for i in other_numerical_fea:
    print('特征%s的分布情况:' % i, bank_churners_data[i].describe())
    plt.figure(figsize=(12, 4))
    plt.subplot(131)                             
    # 不同值较多的特征才画核密度线：
    if bank_churners_data[i].nunique() > 20:
        sns.distplot(bank_churners_data[i])
    else:
        sns.distplot(bank_churners_data[i], kde=False)

    plt.subplot(132)
    sns.boxplot(x='Attrition_Flag', y=i, data=bank_churners_data,
                width=0.7, palette='Set3')

    plt.subplot(133)
    sns.violinplot(x='Attrition_Flag', y=i, data=bank_churners_data, palette='Set3')

    plt.suptitle('特征：%s 对客户流失的影响' % i)
    plt.show()

numerical_fea = ['Customer_Age', 'Dependent_count'] + other_numerical_fea
plt.figure(figsize=(11, 11))
sns.heatmap(bank_churners_data[numerical_fea].corr('pearson'), annot=True, square=True, cmap='RdBu_r',
            linewidths=0.3)
plt.xticks(rotation=-60)
plt.show()    
```
![](https://sc03.alicdn.com/kf/U83bea7f28a644cbca6e413e3867771fbJ.jpg)   
![](https://sc04.alicdn.com/kf/U72e3e113db064312a46ed2c9c0d758986.jpg)  
![](https://ae04.alicdn.com/kf/U4aacaf60a7ff4fb386e7c16e071a95acu.jpg)   
![](https://ae02.alicdn.com/kf/U23f9a07568c849759c53b953bbdaf951i.jpg)  
![](https://sc01.alicdn.com/kf/U2a359c9923154ba38ec13fdc4a54146ay.jpg)   
![](https://ae03.alicdn.com/kf/U025980b97f51409698947ba7516d9a55g.jpg)   
![](https://sc02.alicdn.com/kf/U8303192dfda149868f5f74dd071b5aedX.jpg)   
![](https://sc02.alicdn.com/kf/U4e34db84ed184c2b93cbb2f2b7b7a8b5B.jpg)  
![](https://ae02.alicdn.com/kf/U37e86dd527684ff6a8a37182e9b8db15U.jpg)   
![](https://ae04.alicdn.com/kf/U6b1c759659a547fa8d442cdd708ba3a5A.jpg)  
![](https://ae02.alicdn.com/kf/Uc54b17ae8e0e4656adf83bf2a2ee7cb9m.jpg)  
![](https://ae02.alicdn.com/kf/U42d308fce80147de8e594442fd7e4f8cV.jpg)  
在部分特征里，存在较多异常值。  
其中，在客户分类看到，两种客户的特征分布存在一定的差异。  
这些特征十几个，而且没有太专业的业务了解，暂时都将这些特征保留下来。  
```
numerical_fea = ['Customer_Age', 'Dependent_count'] + other_numerical_fea
plt.figure(figsize=(11, 11))
sns.heatmap(bank_churners_data[numerical_fea].corr('pearson'), annot=True, square=True, cmap='RdBu_r',
            linewidths=0.3)
plt.xticks(rotation=-60)
plt.show()
```
![](https://ae03.alicdn.com/kf/Uda78c982a3c445668170063face4bbf08.jpg)

从热力图看到，
- Total_Trans_Amt 和 Total_Trans_Ct 的相关性达到 0.81
- Months_on_book 和 Customer_Age 相关性达到 0.79
- Total_Revolving_Bal 和 Avg_Utilization_Ratio相关性为0.62
其余数特征之间没有很明显的线性相关关系。


### 3. 数据清洗
首先，将类别型变量做热编码处理：
```
bank_churners_data['Attrition_Flag'] = bank_churners_data['Attrition_Flag'].replace({'Existing Customer': 0, 'Attrited Customer': 1})
bank_churners_data['Gender'] = bank_churners_data['Gender'].replace({'M': 0, 'F': 1})
dummy_fea = ['Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']
dummy_df = pd.DataFrame()
# Card_Category
new_dummy = pd.get_dummies(bank_churners_data['Card_Category'], prefix='Card_Category').drop(columns=['Card_Category_Silver'])
dummy_df = pd.concat([dummy_df, new_dummy], axis=1)
# Education_Level
new_dummy = pd.get_dummies(bank_churners_data['Education_Level'], prefix='Education_Level').drop(columns=['Education_Level_Unknown'])
dummy_df = pd.concat([dummy_df, new_dummy], axis=1)
# Marital_Status
new_dummy = pd.get_dummies(bank_churners_data['Marital_Status'], prefix='Marital_Status').drop(columns=['Marital_Status_Unknown'])
dummy_df = pd.concat([dummy_df, new_dummy], axis=1)
# Income_Category
new_dummy = pd.get_dummies(bank_churners_data['Income_Category'], prefix='Income_Category').drop(columns=['Income_Category_Unknown'])
dummy_df = pd.concat([dummy_df, new_dummy], axis=1)
```
然后，删掉没用的列（已经被get_dummy的特征列），重新组合建模数据：
```
new_bank_churners_data = bank_churners_data.drop(columns=dummy_fea)
new_bank_churners_data = pd.concat([new_bank_churners_data, dummy_df], axis=1)
print(new_bank_churners_data.head(5))
print(new_bank_churners_data.shape)
```
现在数据集大小：(10127, 33)  

下面依次进行：划分训练集测试集、做上采样（解决类别不平衡问题），然后做标准化，再进行建模：
```
X = new_bank_churners_data.iloc[:, 1:]
Y = new_bank_churners_data.iloc[:, 0]
print('X:', X.shape)
print('Y:', Y.shape)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.75, random_state=123)
print('x_train:', x_train.shape)
print('y_train:', y_train.shape)
print('y_train分布：', Counter(y_train))
# SMOTE上采样（仅对训练集，不处理测试集）
smo = SMOTE()
x_smo, y_smo = smo.fit_sample(x_train, y_train)
print('上采样后（SMOTE）标签分布情况：', Counter(y_smo))
# 标准化操作（仅对 X 处理，Y不处理）
scaler = StandardScaler()
scaler.fit(x_smo)
x_smo_scaled = scaler.transform(x_smo)
x_smo_scaled = pd.DataFrame(x_smo_scaled, columns=x_train.columns)
print(x_smo_scaled.head(5))
x_test_scaled = scaler.transform(x_test)
x_test_scaled = pd.DataFrame(x_test_scaled, columns=x_train.columns)
```
现用到的数据有：  
x_smo_scaled、x_test_scaled、y_smo、y_test    

下面用决策树给特征重要性排序，选择合适的特征建模：  
```
rf_select = RandomForestClassifier(random_state=666)
rf_select.fit(x_smo_scaled, y_smo)
fea_importance = rf_select.feature_importances_
index_sorted = np.argsort(fea_importance)  # fea_importance从小到大排列，并提取其对应的index(索引)，后面画图用
print(index_sorted)
# 排行前25特征
plt.figure(figsize=(7, 6))
bar_position = np.arange(25) + 0.5   # 定义条形位置
plt.barh(bar_position, fea_importance[index_sorted][-25:], align='center')
plt.yticks(bar_position, x_smo_scaled.columns[index_sorted][-25:])  # y轴标签
plt.xlabel("属性重要性")
plt.title("前25个重要特征")
plt.tight_layout()
plt.show()
# 排行前20特征
plt.figure(figsize=(7, 6))
bar_position = np.arange(20) + 0.5   # 定义条形位置
plt.barh(bar_position, fea_importance[index_sorted][-20:], align='center')
plt.yticks(bar_position, x_smo_scaled.columns[index_sorted][-20:])  # y轴标签
plt.xlabel("属性重要性")
plt.title("前20个重要特征")
plt.tight_layout()
plt.show()
print('特征重要性：', fea_importance[index_sorted][-4:])
print('前20的特征：', type(x_smo_scaled.columns[index_sorted][-20:]))
print('前20的特征：', x_smo_scaled.columns[index_sorted][-20:])
col_20 = list(x_smo_scaled.columns[index_sorted][-20:])
```
![](https://sc01.alicdn.com/kf/Ub9d9e4d4f4df4b279ea3251c2d49ee06F.jpg)
![](https://ae03.alicdn.com/kf/U34bc67e882394ce091e158055064b641p.jpg)    
可以看到，前4个特征有较明显的作用，分别是:  
Total_Trans_Ct(0.20) > Total_Trans_Amt(0.18) > Total_Revolving_Bal(0.10) > Total_Ct_Chng_Q4_Q1(0.09)
其余特征的作用相对较小。  
现采用前20个特征，其余特征暂时撇除。
```
x_20_train = x_smo_scaled[col_20]
x_20_test = x_test_scaled[col_20]
```
下面建模用到的数据有：  
x_20_train、x_20_test、y_smo、y_test   

###  4. 建模与模型评价分析
用 10 折交叉验证来评估效果：
```
cv_Fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=321)
```
#### 4.1 分别采用KNN、SVM、随机森林、GBDT和xgboost来建模

##### (4.1.1) KNN
n 值设定范围：1-30，采用 f1-score 作为模型效果评估标准
```
param = {'n_neighbors': list(range(1,31))}
knn_clf = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param, cv=cv_Fold, scoring='f1')
knn_clf.fit(x_20_train, y_smo)
best_params = knn_clf.best_params_  # 得出最佳参数组合
best_score = knn_clf.best_score_  # 得出最佳得分
print('knn 各参数组合得分\n', knn_clf.cv_results_['mean_test_score'])  # 得出不同参数组合下10折的一个平均得分，直接用cv_results_可以看出每一折的得分
print('knn 最佳参数', best_params)
print('knn 最高分数：%s' % format(best_score, '.3f'))
```
训练集得出最佳参数及分数：    
1）最佳参数 {'n_neighbors': 6}    
2）最高分数：0.922    
将n=6代入模型，可得：  
```
best_knn = KNeighborsClassifier(n_neighbors=6)
best_knn.fit(x_20_train, y_smo)
y_pred = best_knn.predict(x_20_test)
target_names = ['Existing Customer', 'Attrited Customer']
print('classification_report:\n', classification_report(y_pred, y_test, target_names=target_names))
print('f1_score:', f1_score(y_test, y_pred))
print('accuracy_score:', accuracy_score(y_test, y_pred))
```
得出：  
f1_score: 0.642512077294686  
accuracy_score: 0.8830963665086888  
从f1分数来看，建模效果并不是很好。  


下面用SVM来建模：  
##### (4.1.2) SVM
主要调 C 和 gamma 值，用 GridSearchCV 来解决：  
```
param = {'C':[0.01, 0.1, 1, 10, 100, 1000], 'gamma':[0.01, 0.1, 1, 10, 100, 1000]}
grid_search = GridSearchCV(estimator=SVC(kernel='rbf', cache_size=1000),
                           param_grid=param, cv=cv_Fold,
                           scoring='f1')
grid_search.fit(x_20_train, y_smo)
best_params = grid_search.best_params_  # 得出最佳参数组合
best_score = grid_search.best_score_  # 得出最佳得分
print('SVM各参数组合得分\n', grid_search.cv_results_['mean_test_score'])  # 得出不同参数组合下10折的一个平均得分，直接用cv_results_可以看出每一折的得分
print('SVM最佳参数', best_params)
print('SVM最高分数：%s' % format(best_score, '.3f'))
```
训练集得出最佳参数组合为：{'C': 10, 'gamma': 0.1}  
代入模型：  
```
best_svc = SVC(C=10, kernel='rbf', gamma=0.1)
best_svc.fit(x_20_train, y_smo)
y_pred = best_svc.predict(x_20_test)
print('classification_report:', classification_report(y_test, y_pred))
print('f1_score:', f1_score(y_test, y_pred))
print('accuracy_score:', accuracy_score(y_test, y_pred))
```
f1_score: 0.7582697201017812
accuracy_score: 0.9249605055292259
从f1分数来看，SVM的建模效果也不是很好。


##### (4.1.3) Random Forest 随机森林
主要调节框架参数：n_estimators, 决策树参数：最大特征数 max_features、最大深度 max_depth、内部节点再划分所需最小样本数 min_samples_split 和叶子节点最少样本数 min_samples_leaf。   
(因笔记本计算能力问题，实际只调到两个参数：n_estimators 若学习器个数 和 max_depth 树的深度)   
```
param_test1 = {'n_estimators':range(50,201,10),
               'max_depth':range(3,12)
               # 'min_samples_split':range(70,201,20)
               }
rf_clf = GridSearchCV(estimator=RandomForestClassifier(max_features='sqrt', random_state=999),
                      param_grid=param_test1, scoring='f1', cv=cv_Fold)

rf_clf.fit(x_20_train, y_smo)
best_params = rf_clf.best_params_  # 得出最佳参数组合
best_score = rf_clf.best_score_  # 得出最佳得分
print('random forest 各参数组合得分\n', rf_clf.cv_results_['mean_test_score'])  # 得出不同参数组合下10折的一个平均得分，直接用cv_results_可以看出每一折的得分
print('random forest 最佳参数', best_params)
print('random forest 最高分数：%s' % format(best_score, '.3f'))
```
random forest 最佳参数 {'max_depth': 11, 'n_estimators': 200}    
random forest 最高分数：0.973     
将训练集得出的参数代入测试集：    
```
best_rf = RandomForestClassifier(n_estimators=200, max_depth=11)
best_rf.fit(x_20_train, y_smo)
y_pred = best_rf.predict(x_20_test)
target_names = ['Existing Customer', 'Attrited Customer']
print('classification_report:\n', classification_report(y_pred, y_test, target_names=target_names))
print('f1_score:', f1_score(y_test, y_pred))
print('accuracy_score:', accuracy_score(y_test, y_pred))
 ```
 得出：  
f1_score: 0.8473748473748474  
accuracy_score: 0.9506319115323855  
f1分数达到0.85，比 KNN 和 SVM 效果都好。  


##### (4.1.4) GBDT
考虑参数：    
框架参数：步长(learning rate)和迭代次数(n_estimators)       
弱学习器参数：决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split      
因运算能力，实际只用到步长(learning rate)和迭代次数(n_estimators)：      
```
from sklearn.ensemble import GradientBoostingClassifier


param_1 = {'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5], 'n_estimators':range(50, 200, 10)}
gbdt_1 = GridSearchCV(estimator=GradientBoostingClassifier(max_depth=10, subsample=0.8,   # 用 subsample 提高实验可信度
                                                           min_samples_split=300,
                                                           min_samples_leaf=20,
                                                           random_state=10),
                       param_grid=param_1, scoring='f1', cv=cv_Fold)
gbdt_1.fit(x_20_train, y_smo)
print('random forest 最佳参数', gbdt_1.best_params_)
print('random forest 最高分数：%s' % format(gbdt_1.best_score_, '.3f'))
```
random forest 最佳参数 {'learning_rate': 0.5, 'n_estimators': 160}    
random forest 最高分数：0.986    
代入模型：    
```
best_gbdt = GradientBoostingClassifier(learning_rate=0.5, n_estimators=160,
                                       max_depth=10, subsample=0.8,
                                       min_samples_split=300,
                                       min_samples_leaf=20,
                                       random_state=10
                                       )

best_gbdt.fit(x_20_train, y_smo)
y_pred = best_gbdt.predict(x_20_test)
target_names = ['Existing Customer', 'Attrited Customer']
print('classification_report:\n', classification_report(y_pred, y_test, target_names=target_names))
print('f1_score:', f1_score(y_test, y_pred))
print('accuracy_score:', accuracy_score(y_test, y_pred))
```
得出：  
f1_score: 0.9051833122629583  
accuracy_score: 0.9703791469194313   
相比随机森林，GBDT 的 f1_score 有所上升。  


##### (4.1.5) Xgboost
```
import xgboost as xgb


param_1 = {'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5],
           'n_estimators':range(50, 200, 10)
           }

xgb = GridSearchCV(estimator=xgb.XGBClassifier(booster='gbtree',
                                               objective='binary:logistic',
                                               max_depth=10,
                                               random_state=10),
                   param_grid=param_1, scoring='f1', cv=cv_Fold)
xgb.fit(x_20_train, y_smo)
print('xgboost 最佳参数', xgb.best_params_)
print('xgboost 最高分数：%s' % format(xgb.best_score_, '.3f'))
```
训练集得出：  
xgboost 最佳参数 {'learning_rate': 0.5, 'n_estimators': 190}  
xgboost 最高分数：0.983  
代入模型可得：  
```
best_xgb = xgb.XGBClassifier(booster='gbtree',
                             objective='binary:logistic',
                             max_depth=10,
                             learning_rate=0.5,
                             n_estimators=190
                             )

best_xgb.fit(x_20_train, y_smo)
y_pred = best_xgb.predict(x_20_test)
target_names = ['Existing Customer', 'Attrited Customer']
print('classification_report:\n', classification_report(y_pred, y_test, target_names=target_names))
print('f1_score:', f1_score(y_test, y_pred))
print('accuracy_score:', accuracy_score(y_test, y_pred))
```
f1_score: 0.8936170212765957  
accuracy_score: 0.9664296998420221  
出来的效果没有预想中的好，甚至比没有做任何调参的效果还差一点。对于这块的调参后续还要补一下。    


#### 4.2 对原始数据集做预测
选择 GBDT的模型来对原始数据集做预测：  

```
X_scaled = scaler.transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=x_train.columns)
X_scaled_20 = X_scaled[col_20]
Y_pred_GBDT = best_gbdt.predict(X_scaled_20)
target_names = ['Existing Customer', 'Attrited Customer']
print('原始数据集的 classification_report:\n', classification_report(Y_pred_GBDT, Y, target_names=target_names))
print('原始数据集的 f1_score:', f1_score(Y, Y_pred_GBDT))

ax = sns.heatmap(confusion_matrix(Y_pred_GBDT, Y), annot=True,
                 cmap='coolwarm', fmt='d')
ax.set_title('Prediction On Original Data With GBDT Model Confusion Matrix')
ax.set_xticklabels(['Not Churn', 'Churn'], fontsize=12)
ax.set_yticklabels(['Predicted Not Churn', 'Predicted Churn'], fontsize=12)
plt.tight_layout()
plt.show()
```  
预测结果如下：  
![](https://sc01.alicdn.com/kf/Uc8684153300b410d936b8f47b1f22575y.jpg)  
![](https://ae03.alicdn.com/kf/Ua81c0b360785490abf467fe479b3cd887.jpg)  
从结果来看，预测效果还是不错的，准确度有保证。实际运用的时候，预测效果应该没有这么好，可尝试调参或用 XGBoost 方法来调优。


###  5. 总结
- (1) 因为建模有用到原始数据，所以最后的预测准确度会偏高一点。实际应用中，可尝试多种参数组合，若计算机性能较好，可尝试多个参数。   
- (2) 从 KNN 和 SVM 对测试集的预测结果来看，针对高维数据，这两个方法最好还是别用，存在维度灾难问题。其次， SVM 算法运算相对较慢，如果数据量达到数十万或以上，在实际应用中会影响产品的迭代速度。   
- (3) 后续优化，可考虑用 PCA 来做，让新特征能蕴含更多信息、减小维度灾难，但是模型的解释性就没有这么高。  
- (4) 从运营角度考虑，信用卡业务客户流失原因，可考虑更多因素：
  - 可关注流失客户的流失时间点、流失客户的来源渠道、是否有类似的消费行为或动作、银行是否对信用卡业务作出调整……
  - 内外因素考虑：
    - 内因：信用卡业务是否有调整、与外部商务合适是否终止、产品迭代速度是否合理、产品内部是否形成消费闭环、客户经理服务效率问题、是否根据客户定制个性化服务、是否有自家银行的特色服务(市场同质化凸显)
    - 外因：国家是否出台新政策、经济是否下行、是否其他银行或平台推出新产品（互联网金融，额度更大、优惠力度更大、手续更简便等）、是否有新技术出现(新奇效应或者永久替代)
- (5) 做好客户分群，及时转型，应对当今多变的竞争环境
     可制定合适贵和，划分VIP关注群体、潜力消费群体、一般关注群体、即将流失群体、已经流失群体。高消费、有潜力的群体 ，需要客户经理更多的关注，定制更具有个人特色的服务。面对即将流失群体，可作出有吸引力的优惠政策吸引客户留存。面对已经流失群体，可通过短信、Push通知、客户经理电话联络、邮件等召回用户。
